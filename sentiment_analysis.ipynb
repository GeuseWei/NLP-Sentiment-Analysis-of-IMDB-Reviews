{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17cc2fad",
   "metadata": {},
   "source": [
    "## 1. Import the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae483fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "697b3035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/geuse/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/geuse/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/geuse/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/geuse/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6257eed",
   "metadata": {},
   "source": [
    "## 2. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11861072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reviews(path, sentiment):\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    # Load all the reviews from the directory.\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(path, filename), 'r', encoding='utf-8') as file:\n",
    "                reviews.append(file.read())\n",
    "                labels.append(sentiment)\n",
    "    return reviews, labels\n",
    "\n",
    "\n",
    "def load_imdb_data(base_path):\n",
    "    # Define the paths for positive and negative reviews for both training and testing datasets.\n",
    "    train_pos_path = os.path.join(base_path, 'train', 'pos')\n",
    "    train_neg_path = os.path.join(base_path, 'train', 'neg')\n",
    "    test_pos_path = os.path.join(base_path, 'test', 'pos')\n",
    "    test_neg_path = os.path.join(base_path, 'test', 'neg')\n",
    "\n",
    "    # Load the reviews and labels for each category.\n",
    "    train_pos_reviews, train_pos_labels = load_reviews(train_pos_path, 1)\n",
    "    train_neg_reviews, train_neg_labels = load_reviews(train_neg_path, 0)\n",
    "    test_pos_reviews, test_pos_labels = load_reviews(test_pos_path, 1)\n",
    "    test_neg_reviews, test_neg_labels = load_reviews(test_neg_path, 0)\n",
    "\n",
    "    # Combine the reviews and labels for training and testing datasets.\n",
    "    train_reviews = train_pos_reviews + train_neg_reviews\n",
    "    train_labels = train_pos_labels + train_neg_labels\n",
    "    test_reviews = test_pos_reviews + test_neg_reviews\n",
    "    test_labels = test_pos_labels + test_neg_labels\n",
    "\n",
    "    # Create pandas dataframes from the reviews and labels.\n",
    "    train_data = pd.DataFrame({\n",
    "        'review': train_reviews,\n",
    "        'sentiment': train_labels\n",
    "    })\n",
    "\n",
    "    test_data = pd.DataFrame({\n",
    "        'review': test_reviews,\n",
    "        'sentiment': test_labels\n",
    "    })\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74a420ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"./aclImdb\"\n",
    "train_data, test_data = load_imdb_data(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "852c139b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment\n",
      "0  For a movie that gets no respect there sure ar...          1\n",
      "1  Bizarre horror movie filled with famous faces ...          1\n",
      "2  A solid, if unremarkable film. Matthau, as Ein...          1\n",
      "3  It's a strange feeling to sit alone in a theat...          1\n",
      "4  You probably all already know this by now, but...          1\n",
      "                                              review  sentiment\n",
      "0  Based on an actual story, John Boorman shows t...          1\n",
      "1  This is a gem. As a Film Four production - the...          1\n",
      "2  I really like this show. It has drama, romance...          1\n",
      "3  This is the best 3-D experience Disney has at ...          1\n",
      "4  Of the Korean movies I've seen, only three had...          1\n"
     ]
    }
   ],
   "source": [
    "print(train_data.head())\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa670d3",
   "metadata": {},
   "source": [
    "## 3.  Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72d80caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_review(review):\n",
    "    # Convert to lowercase.\n",
    "    review = review.lower()\n",
    "    # Remove non-alphabet characters.\n",
    "    review = re.sub('[^a-z]', ' ', review)\n",
    "    # Tokenize the review.\n",
    "    words = nltk.word_tokenize(review)\n",
    "    # Remove stop words.\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    # Lemmatize the words.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # Join the preprocessed words back into a string.\n",
    "    review = ' '.join(words)\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229e96e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the reviews in the training and testing datasets.\n",
    "train_data['review'] = train_data['review'].apply(preprocess_review)\n",
    "test_data['review'] = test_data['review'].apply(preprocess_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5af222b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment\n",
      "0  movie get respect sure lot memorable quote lis...          1\n",
      "1  bizarre horror movie filled famous face stolen...          1\n",
      "2  solid unremarkable film matthau einstein wonde...          1\n",
      "3  strange feeling sit alone theater occupied par...          1\n",
      "4  probably already know additional episode never...          1\n",
      "                                              review  sentiment\n",
      "0  based actual story john boorman show struggle ...          1\n",
      "1  gem film four production anticipated quality i...          1\n",
      "2  really like show drama romance comedy rolled o...          1\n",
      "3  best experience disney themeparks certainly be...          1\n",
      "4  korean movie seen three really stuck first exc...          1\n"
     ]
    }
   ],
   "source": [
    "print(train_data.head())\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef15fd2",
   "metadata": {},
   "source": [
    "## 4. Save the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8217b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save train_data.\n",
    "with open('./train_data.pkl', 'wb') as f:\n",
    "    pickle.dump(train_data, f)\n",
    "\n",
    "# Save test_data.\n",
    "with open('./train_data.pkl', 'wb') as f:\n",
    "    pickle.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5333eedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load train_data.\n",
    "with open('./train_data.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "\n",
    "# Load test_data.\n",
    "with open('./test_data.pkl', 'rb') as f:\n",
    "    test_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5a750a",
   "metadata": {},
   "source": [
    "## 5. Model1: TF-IDF + Logistic Regression\n",
    "**TF-IDF Representation:** TF-IDF is a numerical statistic that reflects how important a word is to a document in a collection or corpus. It considers both the frequency of a word in a document (Term Frequency) and the inverse of the frequency of the document that contains the word in the whole corpus (Inverse Document Frequency). The combination of these two creates a balance where words that are too common across documents get lower weights while unique and meaningful words get higher weights. This helps in eliminating the noise created by common words and stopwords, and focusing on words that really matter to the sentiment.\n",
    "\n",
    "**Logistic Regression:** Logistic Regression is a simple yet powerful linear model for binary classification problems. It works well with high dimensional data, making it a good choice for text data which often results in high-dimensional feature vectors (each word or n-gram can be considered a feature). It's also efficient in terms of computation and memory requirements, which can be crucial when dealing with large datasets. Logistic Regression is quite interpretable as the coefficient of each feature in the logistic regression output can be used to infer the 'importance' of each feature in predicting the target variable. In the case of sentiment analysis, it can help us understand which words (or n-grams) are more influential in driving sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c1be824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create TF-IDF vectorizer.\n",
    "vectorizer = TfidfVectorizer(min_df=5, ngram_range=(1, 2))\n",
    "\n",
    "# Fit the vectorizer to the training set reviews and transform them to vectors.\n",
    "train_features = vectorizer.fit_transform(train_data['review'])\n",
    "\n",
    "# Transform the test set reviews to vectors using the same vectorizer.\n",
    "test_features = vectorizer.transform(test_data['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bcff453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(solver='liblinear')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a Logistic Regression model.\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Train the model with the training set features and labels.¥\n",
    "model.fit(train_features, train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb903088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train accuracy of the Logistic Regression model is: 0.94528\n",
      "The train loss of the Logistic Regression model is: 0.26181187270627615\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Calculate the training accuracy using the model's score() method.\n",
    "train_accuracy = model.score(train_features, train_data['sentiment'])\n",
    "\n",
    "# Print the training accuracy.\n",
    "print('The train accuracy of the Logistic Regression model is:', train_accuracy)\n",
    "\n",
    "# Calculate the training loss.\n",
    "train_prob = model.predict_proba(train_features)\n",
    "train_loss = log_loss(train_data['sentiment'], train_prob)\n",
    "\n",
    "# Print the training loss.\n",
    "print('The train loss of the Logistic Regression model is:', train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "195ce308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy of the Logistic Regression model is: 0.88604\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Use the trained model to predict the sentiment of the test set reviews.\n",
    "predictions = model.predict(test_features)\n",
    "\n",
    "# Calculate the prediction accuracy.\n",
    "accuracy = accuracy_score(test_data['sentiment'], predictions)\n",
    "\n",
    "print('The test accuracy of the Logistic Regression model is:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfefab85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model.\n",
    "joblib.dump(model, 'logistic_regression_model.pkl')\n",
    "\n",
    "# Save the vectorizer.\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4080e46",
   "metadata": {},
   "source": [
    "**Comment:** The training accuracy is quite high at 0.94528 indicating the model is performing well on the training set. The test accuracy, however, is slightly lower at 0.88604. This could suggest a degree of overfitting, where the model has learned the training data very well but doesn't generalize as well to unseen data. The train loss is relatively low at 0.26181, also suggesting good performance on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7707c1ef",
   "metadata": {},
   "source": [
    "## 6. Model 2: TF-IDF + SVM\n",
    "**Support Vector Machine (SVM):** SVM is a powerful machine learning model for classification tasks. It is designed to find the best hyperplane or decision boundary that can separate classes in a higher-dimensional space, which is ideal for high-dimensional data like text data. SVMs are effective in high-dimensional spaces, even in cases where the number of dimensions exceeds the number of samples. This makes them a good choice for text classification problems where each word or n-gram can be considered as a separate dimension.\n",
    "\n",
    "**Handling Non-linearities:** Unlike Logistic Regression, SVM can easily handle non-linear decision boundaries thanks to the kernel trick. This means it can model more complex relationships between your data points, which could lead to better performance in some cases.\n",
    "\n",
    "**Robustness:** SVMs are also robust against overfitting, especially in high-dimensional space. This is because SVMs aim to maximize the margin, i.e., the distance between the decision boundary and the closest points of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d17752f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# Create a SVM model.\n",
    "model = svm.SVC(kernel='linear')\n",
    "\n",
    "# Train the model with the training set features and labels.\n",
    "model.fit(train_features, train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e5708a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train accuracy of the SVM model is: 0.9818\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = joblib.load('svm_model.pkl')\n",
    "\n",
    "train_accuracy = model.score(train_features, train_data['sentiment'])\n",
    "\n",
    "# Print the training accuracy.\n",
    "print('The train accuracy of the SVM model is:', train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ed84eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy of the SVM model is: 0.88652\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict the sentiment of the test set reviews using the SVM model.\n",
    "predictions = model.predict(test_features)\n",
    "\n",
    "# Calculate the prediction accuracy.\n",
    "accuracy = accuracy_score(test_data['sentiment'], predictions)\n",
    "\n",
    "print('The test accuracy of the SVM model is:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5f66326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_model.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, 'svm_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56d187e",
   "metadata": {},
   "source": [
    "**Comment:** The SVM model has even higher training accuracy at 0.9818, suggesting it's performing very well on the training data. The test accuracy is a little lower than the training accuracy but slightly higher than the Logistic Regression model's test accuracy, at 0.88652. This also indicates a bit of overfitting but suggests that the SVM model may generalize slightly better than the Logistic Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc79b0f",
   "metadata": {},
   "source": [
    "## 7. Model 3: LSTM\n",
    "**Sequence Understanding:** Text data is essentially a sequence of words. LSTMs are designed to handle such sequence data, as they can understand the context by remembering or forgetting information with the help of their gating mechanisms (input gate, forget gate and output gate). This ability to \"remember\" previous data in the sequence helps to capture the context and eliminate the problem of long-term dependencies that exist in text data.\n",
    "\n",
    "**Context Capturing:** Unlike traditional machine learning algorithms like Logistic Regression or SVM, LSTM models take into account the entire context of a sentence, not just individual words or n-grams. This allows them to understand nuances in language that can greatly affect sentiment, such as sarcasm or negations.\n",
    "\n",
    "**Handling Variable-Length Sequences:** LSTMs can handle variable-length sequences, meaning they can process reviews of different lengths without needing to predefine the sequence length, unlike traditional machine learning models where a fixed number of features must be set.\n",
    "\n",
    "**Model Complexity:** LSTM networks are capable of modeling complex patterns and interactions between words, which can be particularly useful in sentiment analysis where the meaning of a particular word can depend heavily on its surrounding words.\n",
    "\n",
    "**End-to-End Learning:** LSTM allows for end-to-end learning, where the model learns the best representation for the task at hand by itself. You do not have to manually engineer features (like TF-IDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a1ce0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Set the maximum size of the vocabulary.\n",
    "max_words = 10000\n",
    "# Set the maximum length for each review.\n",
    "max_len = 100\n",
    "\n",
    "# Create a tokenizer, set the maximum size of the vocabulary.\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "# Fit the tokenizer using the training set reviews.\n",
    "tokenizer.fit_on_texts(train_data['review'])\n",
    "\n",
    "# Transform the reviews to sequences of integers using the tokenizer.\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data['review'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['review'])\n",
    "\n",
    "# Pad or truncate the sequences to the same length.\n",
    "train_sequences = pad_sequences(train_sequences, maxlen=max_len)\n",
    "test_sequences = pad_sequences(test_sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5264dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "157/157 [==============================] - 7s 36ms/step - loss: 0.5670 - acc: 0.7133 - val_loss: 0.8029 - val_acc: 0.6108\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 5s 33ms/step - loss: 0.3284 - acc: 0.8715 - val_loss: 0.6344 - val_acc: 0.7034\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.2583 - acc: 0.9024 - val_loss: 0.3663 - val_acc: 0.8406\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 5s 33ms/step - loss: 0.2188 - acc: 0.9205 - val_loss: 0.3751 - val_acc: 0.8316\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 5s 34ms/step - loss: 0.1965 - acc: 0.9288 - val_loss: 0.2703 - val_acc: 0.8838\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 5s 34ms/step - loss: 0.1766 - acc: 0.9372 - val_loss: 0.6559 - val_acc: 0.7606\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 5s 35ms/step - loss: 0.1587 - acc: 0.9441 - val_loss: 0.9602 - val_acc: 0.6928\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 5s 34ms/step - loss: 0.1509 - acc: 0.9471 - val_loss: 0.5107 - val_acc: 0.8332\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 5s 34ms/step - loss: 0.1410 - acc: 0.9512 - val_loss: 0.2891 - val_acc: 0.8928\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 5s 34ms/step - loss: 0.1278 - acc: 0.9560 - val_loss: 0.5117 - val_acc: 0.8222\n",
      "The final training accuracy of the LSTM model is: 0.9559500217437744\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, BatchNormalization, Dropout\n",
    "from keras import regularizers\n",
    "\n",
    "# Define a LSTM model.\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 50, input_length=max_len))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Train the model.\n",
    "history = model.fit(train_sequences, train_data['sentiment'], epochs=10, batch_size=128, validation_split=0.2)\n",
    "\n",
    "# Retrieve the accuracy history from the training process.\n",
    "accuracy_history = history.history['acc']\n",
    "\n",
    "# Get the training accuracy of the last epoch.\n",
    "final_training_accuracy = accuracy_history[-1]\n",
    "\n",
    "# Print the final training accuracy.\n",
    "print('The final training accuracy of the LSTM model is:', final_training_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb3701e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 4s 5ms/step - loss: 0.4084 - acc: 0.8497\n",
      "The test accuracy of the LSTM model is: 0.8497200012207031\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_sequences, test_data['sentiment'])\n",
    "print('The test accuracy of the LSTM model is:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362a71b2",
   "metadata": {},
   "source": [
    "**Comment:** The LSTM model shows a different pattern. During the 10 epochs of training, we can observe a steady decrease in the training loss and a steady increase in the training accuracy, suggesting that the model is continuously learning and improving its performance. The final training accuracy is 0.9560, which is similar to the Logistic Regression and SVM models. However, the test accuracy is slightly lower than the other models, at 0.8497. The validation loss and accuracy fluctuate across epochs, suggesting that the model might be overfitting to the training data and may not generalize as well to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7861b08c",
   "metadata": {},
   "source": [
    "## 8. Conclusion:\n",
    "In conclusion, all three models performed well on the training data with high accuracy scores. They also all exhibited signs of overfitting to varying degrees, as indicated by lower test accuracy scores compared to training accuracy. It would be beneficial to apply strategies to reduce overfitting, such as regularization, early stopping, or more extensive data augmentation.\n",
    "\n",
    "If interpretability is important, Logistic Regression might be the best choice as it allows us to understand the importance of each feature in the prediction. SVMs and LSTMs are less interpretable but might provide better performance, especially on more complex tasks or larger datasets. Finally, LSTMs are particularly well-suited to sequence data and might outperform the other models on tasks where the order of words is important. However, LSTMs are more computationally expensive to train and use for prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
